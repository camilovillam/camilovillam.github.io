---
layout: blog
title: Why you should implement dbt at your organization
date: 2025-12-23
author: Camilo Villa
---


### A familiar story

Have you ever asked different team members for some reports and got conflicting information? 
KPIs that don’t add up or feel inaccurate? 
Or did your team just take too long to send you those reports? 
After that, do you trust your data insights? 
Should it be _that_ hard? 

### Transactional data and analytical data 

Broadly speaking, data can be classified in two categories: Data for transactional purposes, and Data for analytical purposes. 

Data for transactional purposes is the data that you use to run your organization and your business: orders, payments, inventory movements, user actions, customer information. It is data that lives in your CRM, ERP, manufacturing, human resources, billing, your online store, this type of systems. 

But you can also use that same data with a different purpose: for analysis: You want to know which products and services are more profitable, which resources are driving your costs up, what are the bottlenecks in your processes. This helps you understand your business to decide what to do next. 

This is the second type of data we’re referring to: Data for Analytical Purposes, that is, data that supports your decision making based on evidence, on insights, on historical information, even on predictions. 

The source for both types of data might be the same, those systems we mentioned, but since the purpose of data is different, the treatment should be different. For analysis, you might want to aggregate that data, clean it, group it, classify it. 

You might need to extract data from those systems, load it into an analytical database, and then TRANSFORM it to achieve that analysis. We’ll get back to this. 

Keep in mind that this differentiation of transactional and analytical data is not always a strict classification, and the boundaries are constantly and increasingly getting blurred. But it is still a useful mental model and it fits our purpose. 
For the rest of this post, we will focus on analytical data. 


### Importance of data analysis

It is the end of 2025 and you might be thinking that with AI you don’t need to worry too much about processing and managing that analytical data. Surely AI can organize everything, explore your data, analyze it, and give you those important insights. Right? 

Well, in the end, AI depends on that data. Even the most advanced AI model or tool won’t perform well if the input data, the grounding data, the context data, the data used to train those models is not good. Probably you’ve heard of _garbage in, garbage out_. 

The cost of not having systematic processes for treating your analytical data is that you lose your ability to differentiate yourself from your competition by not being just another organization using generic AI tools. Instead, you can have data on which you can trust, from which you can get reliable insights, take decisions, and, very importantly, provide context and grounding data for those AI models. 


### How to scale and improve your data analytics operations

You, your executive team, your business units: you all need reports, dashboards, insights. But there is rarely a “single report that serves all purposes”. More commonly, you need different reports, with different details, indicators, aggregation levels, visualization options, etc. 

This is where things can get messy and complicated: data comes from different sources, in different shapes and formats, and it must serve a variety of teams and purposes.

You then need to implement reliable processes for data analytics, that empower your teams to conduct the analysis they need, but with the confidence that the whole organization is using the same basis of data, "a single source of truth". That no matter which reports you run, you’re not getting conflicting information. 

The process for implementing this flexible, reliable data analysis system usually involves **extracting** the data from its sources, **loading** it into an analytical database, such as a data warehouse, and **transforming** it through different stages to expose it in reports and dashboards that serve your different teams and users. 

Extract. Load. Transform. This is commonly referred to as ELT. 

![Data pipeline diagram](https://github.com/camilovillam/camilovillam.github.io/blob/main/assets/img/blog/pipeline_diagram_new.png?raw=true)

At the heart of this pipeline is transformation, the T, and here, dbt steps in.


### dbt: What it is

Let me recap briefly. 

By now, you understand the importance of data and the importance of having a sophisticated, well\-functioning data analytics operation inside your organization to serve your different teams and users. 

What I now want to recommend you is the tool: **dbt**. 

**Data build tool**. 

dbt is an open-source tool that can support you with your goal of transforming data for analytical purposes by bringing together:

- The most broadly available and well\-known language for data querying and manipulation: SQL,
- Jinja macros, which is just a way to include templates and repeating code into SQL, giving it extra capabilities,
- The ability to configure and run automated testing of our transformations,
- An easy way to generate documentation,
- Source control of our code, which the ability to track changes, integrate code written by different developers enhancing collaboration, revert those changes if needed, etc.,
- And a way to continuously integrate and deploy our code from development environments into production. 

If you have ever heard about software development operations \(“DevOps”\), what I just mentioned will sound familiar, but with a twist: DataOps. 

In a nutshell, DataOps is bringing these well\-established practices of Software Development to Data Analytics. In this way, we treat data analytics as what it really is: a software product well\-deserving of systematic care: developed in collaboration, with clear owners, with versions, releases, quality control, among others. 

dbt as a tool enables and facilitates DataOps for your data transformations and to treat your data as a reliable product. 


### Wide adoption and a strong ecosystem

In addition to what I just mentioned, dbt has several additional benefits: 

dbt is widely used in different industries and in organizations and projects of all sizes, showing its versatility and capabilities. dbt’s ecosystem is strong, with an active community of users, certifications, consultants, talent, tutorials, best practices, all readily available. 

dbt also integrates well with a huge array of data warehouses and databases, including Snowflake, Redshift, BigQuery, etc., as well as with your preferred BI tool. Thanks to this, dbt won’t force you to pick specific technologies or to lock into a certain vendor, leaving you with the valuable flexibility of configuring and evolving the different components of your complete data stack as it best suits you. 

### Let me summarize

dbt is a tool that can support your organization with improving and modernizing your data analytics operations. It will allow your organization to treat your analytical data as a strategic product, with high standards for collaborative development, testing, documentation, versioning, and deployment. This will enable you and your whole organization to extract insights and reports that are reliable, that can be expanded with confidence, and that are tailored to the specific needs of your different teams. This will also let you have top quality data to ground and improve your AI tools and models. And all this with all the advantages of a widely used, open\-source tool, with a massive pool of talent. 

I hope that by now you are convinced of the value that dbt can bring to your data analytics and why you should implement it at your organization. I invite you to explore it further and seriously consider it as your tool of choice for transforming your data. 


_This post is also available in video format [here](https://youtu.be/pYsobwkkROM){:target="_blank"}_

---
_If you want to keep exploring and see dbt in action in a simple data transformation project, check out this blog and its accompanying video: [An Introduction to dbt](https://camilovilla.net/blog/an-introduction-to-dbt){:target="_blank"}_

